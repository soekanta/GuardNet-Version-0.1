{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "header"
            },
            "source": [
                "# GuardNet: FIXED Pipeline Training (No Data Leakage)\n",
                "\n",
                "This notebook trains phishing detection models using URL-only features with proper sklearn Pipelines to prevent data leakage.\n",
                "\n",
                "---\n",
                "\n",
                "## Cell 1: Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install_deps"
            },
            "outputs": [],
            "source": [
                "!pip install -q pandas numpy matplotlib seaborn scikit-learn"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "import_header"
            },
            "source": [
                "## Cell 2: Import Libraries"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "imports"
            },
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, learning_curve\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.metrics import (\n",
                "    confusion_matrix, roc_curve, auc, precision_recall_curve,\n",
                "    accuracy_score, precision_score, recall_score, f1_score, average_precision_score\n",
                ")\n",
                "import json\n",
                "import pickle\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "plt.style.use('seaborn-v0_8-darkgrid')\n",
                "sns.set_palette(\"husl\")\n",
                "\n",
                "print(\"‚úÖ All libraries imported successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "upload_header"
            },
            "source": [
                "## Cell 3: Upload Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "upload_dataset"
            },
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "import os\n",
                "\n",
                "print(\"üì§ Please upload your dataset file (PhiUSIIL_Phishing_URL_Dataset.csv)\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "dataset_file = list(uploaded.keys())[0]\n",
                "print(f\"\\n‚úÖ Dataset uploaded: {dataset_file}\")\n",
                "print(f\"   File size: {os.path.getsize(dataset_file) / (1024*1024):.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "load_header"
            },
            "source": [
                "## Cell 4: Load Data & Feature Separation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load_data"
            },
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"GuardNet: FIXED Pipeline Training (No Data Leakage)\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(\"\\n[1/12] Loading dataset...\")\n",
                "df = pd.read_csv(dataset_file)\n",
                "print(f\"Dataset shape: {df.shape}\")\n",
                "print(f\"Total samples: {df.shape[0]:,}\")\n",
                "\n",
                "print(\"\\n[2/12] Separating URL-only features from HTML features...\")\n",
                "\n",
                "# URL-only features (static, extracted from URL string only)\n",
                "url_features = [\n",
                "    'URLLength', 'DomainLength', 'IsDomainIP', 'TLDLength',\n",
                "    'NoOfSubDomain', 'HasObfuscation', 'NoOfObfuscatedChar', 'ObfuscationRatio',\n",
                "    'NoOfLettersInURL', 'LetterRatioInURL', 'NoOfDegitsInURL', 'DegitRatioInURL',\n",
                "    'NoOfEqualsInURL', 'NoOfQMarkInURL', 'NoOfAmpersandInURL', 'NoOfOtherSpecialCharsInURL',\n",
                "    'SpacialCharRatioInURL', 'IsHTTPS', 'URLSimilarityIndex', 'CharContinuationRate',\n",
                "    'TLDLegitimateProb', 'URLCharProb'\n",
                "]\n",
                "\n",
                "existing_url_features = [f for f in url_features if f in df.columns]\n",
                "print(f\"  URL-only features: {len(existing_url_features)}\")\n",
                "\n",
                "X_url = df[existing_url_features]\n",
                "y = df['label']\n",
                "\n",
                "class_counts = y.value_counts()\n",
                "print(f\"\\nüìä Class Distribution:\")\n",
                "print(f\"  Legitimate (1): {class_counts[1]:,} ({class_counts[1]/len(y)*100:.2f}%)\")\n",
                "print(f\"  Phishing (0):   {class_counts[0]:,} ({class_counts[0]/len(y)*100:.2f}%)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "viz1_header"
            },
            "source": [
                "## Cell 5: Visualization - Class Distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "viz_class_dist"
            },
            "outputs": [],
            "source": [
                "print(\"\\n[3/12] Creating class distribution visualization...\")\n",
                "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "colors = ['#ff6b6b', '#51cf66']\n",
                "ax1.pie(class_counts.values, labels=['Phishing', 'Legitimate'], autopct='%1.1f%%',\n",
                "        colors=colors, startangle=90, textprops={'fontsize': 12, 'weight': 'bold'})\n",
                "ax1.set_title('Dataset Class Distribution', fontsize=14, weight='bold', pad=20)\n",
                "\n",
                "bars = ax2.bar(['Phishing', 'Legitimate'], class_counts.values, color=colors, alpha=0.8, edgecolor='black')\n",
                "ax2.set_ylabel('Number of Samples', fontsize=12, weight='bold')\n",
                "ax2.set_title('Sample Count by Class', fontsize=14, weight='bold', pad=20)\n",
                "ax2.grid(axis='y', alpha=0.3)\n",
                "for bar in bars:\n",
                "    height = bar.get_height()\n",
                "    ax2.text(bar.get_x() + bar.get_width()/2., height,\n",
                "             f'{int(height):,}', ha='center', va='bottom', fontsize=11, weight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('01_class_distribution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"   ‚úÖ Saved: 01_class_distribution.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "split_header"
            },
            "source": [
                "## Cell 6: Split Data & Create Pipelines"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "split_data"
            },
            "outputs": [],
            "source": [
                "print(\"\\n[4/12] Splitting dataset...\")\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X_url, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "print(f\"  Training set: {X_train.shape[0]:,} samples\")\n",
                "print(f\"  Test set:     {X_test.shape[0]:,} samples\")\n",
                "print(f\"  Features:     {X_train.shape[1]} (URL-only)\")\n",
                "\n",
                "print(\"\\n[5/12] Creating sklearn Pipelines (FIXED - no leakage)...\")\n",
                "\n",
                "lr_pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('classifier', LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1))\n",
                "])\n",
                "\n",
                "rf_pipeline = Pipeline([\n",
                "    ('scaler', StandardScaler()),\n",
                "    ('classifier', RandomForestClassifier(\n",
                "        n_estimators=100, max_depth=20, min_samples_split=5,\n",
                "        min_samples_leaf=2, random_state=42, n_jobs=-1\n",
                "    ))\n",
                "])\n",
                "\n",
                "print(\"   ‚úÖ Pipelines created (scaler + model)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "train_header"
            },
            "source": [
                "## Cell 7: Train Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "train_models"
            },
            "outputs": [],
            "source": [
                "print(\"\\n[6/12] Training models...\")\n",
                "\n",
                "print(\"  üîµ Training Logistic Regression Pipeline...\")\n",
                "lr_pipeline.fit(X_train, y_train)\n",
                "print(\"     ‚úÖ LR Pipeline trained\")\n",
                "\n",
                "print(\"  üå≤ Training Random Forest Pipeline...\")\n",
                "rf_pipeline.fit(X_train, y_train)\n",
                "print(\"     ‚úÖ RF Pipeline trained\")\n",
                "\n",
                "with open('lr_pipeline.pkl', 'wb') as f:\n",
                "    pickle.dump(lr_pipeline, f)\n",
                "with open('rf_pipeline.pkl', 'wb') as f:\n",
                "    pickle.dump(rf_pipeline, f)\n",
                "print(\"   ‚úÖ Pipelines saved\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "predict_header"
            },
            "source": [
                "## Cell 8: Generate Predictions & Calculate Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "predictions"
            },
            "outputs": [],
            "source": [
                "print(\"\\n[7/12] Generating predictions...\")\n",
                "y_pred_lr = lr_pipeline.predict(X_test)\n",
                "y_pred_rf = rf_pipeline.predict(X_test)\n",
                "y_proba_lr = lr_pipeline.predict_proba(X_test)[:, 1]\n",
                "y_proba_rf = rf_pipeline.predict_proba(X_test)[:, 1]\n",
                "\n",
                "y_proba_hybrid = (y_proba_lr + y_proba_rf) / 2\n",
                "y_pred_hybrid = (y_proba_hybrid >= 0.5).astype(int)\n",
                "\n",
                "print(\"\\n[8/12] Calculating performance metrics...\")\n",
                "models = {\n",
                "    'Logistic Regression': (y_pred_lr, y_proba_lr),\n",
                "    'Random Forest': (y_pred_rf, y_proba_rf),\n",
                "    'Hybrid Model': (y_pred_hybrid, y_proba_hybrid)\n",
                "}\n",
                "\n",
                "metrics_results = {}\n",
                "for model_name, (y_pred, y_proba) in models.items():\n",
                "    metrics_results[model_name] = {\n",
                "        'Accuracy': accuracy_score(y_test, y_pred),\n",
                "        'Precision': precision_score(y_test, y_pred),\n",
                "        'Recall': recall_score(y_test, y_pred),\n",
                "        'F1-Score': f1_score(y_test, y_pred),\n",
                "        'AUC': auc(*roc_curve(y_test, y_proba)[:2])\n",
                "    }\n",
                "    print(f\"\\n  üìä {model_name}:\")\n",
                "    for metric, value in metrics_results[model_name].items():\n",
                "        print(f\"    {metric:12s}: {value:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "viz2_header"
            },
            "source": [
                "## Cell 9: Visualization - Performance Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "viz_metrics"
            },
            "outputs": [],
            "source": [
                "print(\"\\n[9/12] Creating performance metrics comparison...\")\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "\n",
                "metrics_df = pd.DataFrame(metrics_results).T\n",
                "x = np.arange(len(metrics_df.columns))\n",
                "width = 0.25\n",
                "\n",
                "colors = ['#3498db', '#2ecc71', '#e74c3c']\n",
                "for i, (model, color) in enumerate(zip(metrics_df.index, colors)):\n",
                "    offset = width * (i - 1)\n",
                "    bars = ax.bar(x + offset, metrics_df.loc[model], width, \n",
                "                   label=model, color=color, alpha=0.8, edgecolor='black')\n",
                "    for bar in bars:\n",
                "        height = bar.get_height()\n",
                "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
                "                f'{height:.3f}', ha='center', va='bottom', fontsize=9, weight='bold')\n",
                "\n",
                "ax.set_xlabel('Metrics', fontsize=12, weight='bold')\n",
                "ax.set_ylabel('Score', fontsize=12, weight='bold')\n",
                "ax.set_title('Model Performance Comparison (URL-only Features)', fontsize=14, weight='bold', pad=20)\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(metrics_df.columns, fontsize=11)\n",
                "ax.legend(fontsize=11, loc='lower right')\n",
                "ax.set_ylim(0, 1.1)\n",
                "ax.grid(axis='y', alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('02_performance_metrics.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"   ‚úÖ Saved: 02_performance_metrics.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "viz3_header"
            },
            "source": [
                "## Cell 10: Visualization - Confusion Matrices"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "viz_confusion"
            },
            "outputs": [],
            "source": [
                "print(\"\\n[10/12] Creating confusion matrices...\")\n",
                "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
                "\n",
                "for idx, (model_name, (y_pred, _)) in enumerate(models.items()):\n",
                "    cm = confusion_matrix(y_test, y_pred)\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
                "                cbar_kws={'label': 'Count'}, annot_kws={'size': 14, 'weight': 'bold'})\n",
                "    axes[idx].set_title(f'{model_name}\\nConfusion Matrix', fontsize=12, weight='bold')\n",
                "    axes[idx].set_ylabel('True Label', fontsize=11, weight='bold')\n",
                "    axes[idx].set_xlabel('Predicted Label', fontsize=11, weight='bold')\n",
                "    axes[idx].set_xticklabels(['Phishing', 'Legitimate'])\n",
                "    axes[idx].set_yticklabels(['Phishing', 'Legitimate'])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('03_confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"   ‚úÖ Saved: 03_confusion_matrices.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "viz4_header"
            },
            "source": [
                "## Cell 11: Visualization - ROC Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "viz_roc"
            },
            "outputs": [],
            "source": [
                "print(\"\\n[11/12] Creating ROC curves...\")\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "\n",
                "colors_roc = ['#3498db', '#2ecc71', '#e74c3c']\n",
                "for (model_name, (_, y_proba)), color in zip(models.items(), colors_roc):\n",
                "    fpr, tpr, _ = roc_curve(y_test, y_proba)\n",
                "    roc_auc = auc(fpr, tpr)\n",
                "    ax.plot(fpr, tpr, color=color, lw=2.5, label=f'{model_name} (AUC = {roc_auc:.4f})')\n",
                "\n",
                "ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier (AUC = 0.5000)')\n",
                "ax.set_xlim([0.0, 1.0])\n",
                "ax.set_ylim([0.0, 1.05])\n",
                "ax.set_xlabel('False Positive Rate', fontsize=12, weight='bold')\n",
                "ax.set_ylabel('True Positive Rate', fontsize=12, weight='bold')\n",
                "ax.set_title('ROC Curves - URL-only Features', fontsize=14, weight='bold', pad=20)\n",
                "ax.legend(loc='lower right', fontsize=11)\n",
                "ax.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('04_roc_curves.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"   ‚úÖ Saved: 04_roc_curves.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "viz5_header"
            },
            "source": [
                "## Cell 12: Visualization - Precision-Recall Curves"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "viz_pr"
            },
            "outputs": [],
            "source": [
                "print(\"\\n[12/12] Creating Precision-Recall curves...\")\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "\n",
                "for (model_name, (_, y_proba)), color in zip(models.items(), colors_roc):\n",
                "    precision, recall, _ = precision_recall_curve(y_test, y_proba)\n",
                "    avg_precision = average_precision_score(y_test, y_proba)\n",
                "    ax.plot(recall, precision, color=color, lw=2.5,\n",
                "            label=f'{model_name} (AP = {avg_precision:.4f})')\n",
                "\n",
                "ax.set_xlim([0.0, 1.0])\n",
                "ax.set_ylim([0.0, 1.05])\n",
                "ax.set_xlabel('Recall', fontsize=12, weight='bold')\n",
                "ax.set_ylabel('Precision', fontsize=12, weight='bold')\n",
                "ax.set_title('Precision-Recall Curves - URL-only Features', fontsize=14, weight='bold', pad=20)\n",
                "ax.legend(loc='lower left', fontsize=11)\n",
                "ax.grid(alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('05_precision_recall_curves.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"   ‚úÖ Saved: 05_precision_recall_curves.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "viz6_header"
            },
            "source": [
                "## Cell 13: Visualization - Feature Importance"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "viz_feature_imp"
            },
            "outputs": [],
            "source": [
                "print(\"\\nCreating feature importance plot...\")\n",
                "rf_model = rf_pipeline.named_steps['classifier']\n",
                "feature_importance = pd.DataFrame({\n",
                "    'feature': X_train.columns,\n",
                "    'importance': rf_model.feature_importances_\n",
                "}).sort_values('importance', ascending=False).head(20)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 8))\n",
                "bars = ax.barh(range(len(feature_importance)), feature_importance['importance'], \n",
                "               color='#2ecc71', alpha=0.8, edgecolor='black')\n",
                "ax.set_yticks(range(len(feature_importance)))\n",
                "ax.set_yticklabels(feature_importance['feature'], fontsize=10)\n",
                "ax.set_xlabel('Importance Score', fontsize=12, weight='bold')\n",
                "ax.set_title('Top 20 Feature Importance (Random Forest, URL-only)', fontsize=14, weight='bold', pad=20)\n",
                "ax.invert_yaxis()\n",
                "ax.grid(axis='x', alpha=0.3)\n",
                "\n",
                "for i, bar in enumerate(bars):\n",
                "    width = bar.get_width()\n",
                "    ax.text(width, bar.get_y() + bar.get_height()/2.,\n",
                "            f'{width:.4f}', ha='left', va='center', fontsize=9, weight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('06_feature_importance.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"   ‚úÖ Saved: 06_feature_importance.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "viz7_header"
            },
            "source": [
                "## Cell 14: Visualization - Learning Curve (FIXED)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "viz_learning"
            },
            "outputs": [],
            "source": [
                "print(\"\\nCreating learning curve (FIXED - using Pipeline)...\")\n",
                "train_sizes, train_scores, val_scores = learning_curve(\n",
                "    rf_pipeline, X_train, y_train,\n",
                "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
                "    cv=5, n_jobs=-1, random_state=42\n",
                ")\n",
                "\n",
                "train_mean = np.mean(train_scores, axis=1)\n",
                "train_std = np.std(train_scores, axis=1)\n",
                "val_mean = np.mean(val_scores, axis=1)\n",
                "val_std = np.std(val_scores, axis=1)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "ax.plot(train_sizes, train_mean, 'o-', color='#3498db', lw=2.5, label='Training Score')\n",
                "ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.2, color='#3498db')\n",
                "ax.plot(train_sizes, val_mean, 'o-', color='#e74c3c', lw=2.5, label='Validation Score')\n",
                "ax.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.2, color='#e74c3c')\n",
                "\n",
                "ax.set_xlabel('Training Set Size', fontsize=12, weight='bold')\n",
                "ax.set_ylabel('Score', fontsize=12, weight='bold')\n",
                "ax.set_title('Learning Curve (Random Forest, FIXED Pipeline)', fontsize=14, weight='bold', pad=20)\n",
                "ax.legend(loc='lower right', fontsize=11)\n",
                "ax.grid(alpha=0.3)\n",
                "ax.set_ylim(0.7, 1.05)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('07_learning_curve.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"   ‚úÖ Saved: 07_learning_curve.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "viz8_header"
            },
            "source": [
                "## Cell 15: Visualization - Score Distribution"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "viz_score_dist"
            },
            "outputs": [],
            "source": [
                "print(\"\\nCreating score distribution plot...\")\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
                "\n",
                "phishing_scores = y_proba_hybrid[y_test == 0]\n",
                "legitimate_scores = y_proba_hybrid[y_test == 1]\n",
                "\n",
                "axes[0].hist(phishing_scores, bins=50, alpha=0.7, color='#ff6b6b', label='Phishing (Actual)', edgecolor='black')\n",
                "axes[0].hist(legitimate_scores, bins=50, alpha=0.7, color='#51cf66', label='Legitimate (Actual)', edgecolor='black')\n",
                "axes[0].axvline(x=0.5, color='black', linestyle='--', lw=2, label='Decision Threshold')\n",
                "axes[0].set_xlabel('Prediction Probability', fontsize=12, weight='bold')\n",
                "axes[0].set_ylabel('Frequency', fontsize=12, weight='bold')\n",
                "axes[0].set_title('Prediction Score Distribution (Hybrid Model)', fontsize=13, weight='bold')\n",
                "axes[0].legend(fontsize=11)\n",
                "axes[0].grid(alpha=0.3)\n",
                "\n",
                "data_to_plot = [phishing_scores, legitimate_scores]\n",
                "bp = axes[1].boxplot(data_to_plot, labels=['Phishing', 'Legitimate'], patch_artist=True, widths=0.6)\n",
                "colors = ['#ff6b6b', '#51cf66']\n",
                "for patch, color in zip(bp['boxes'], colors):\n",
                "    patch.set_facecolor(color)\n",
                "    patch.set_alpha(0.7)\n",
                "axes[1].axhline(y=0.5, color='black', linestyle='--', lw=2, label='Decision Threshold')\n",
                "axes[1].set_ylabel('Prediction Probability', fontsize=12, weight='bold')\n",
                "axes[1].set_title('Score Distribution by Actual Class', fontsize=13, weight='bold')\n",
                "axes[1].legend(fontsize=11)\n",
                "axes[1].grid(alpha=0.3, axis='y')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('08_score_distribution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"   ‚úÖ Saved: 08_score_distribution.png\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "viz9_header"
            },
            "source": [
                "## Cell 16: Cross-Validation (FIXED)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "cross_val"
            },
            "outputs": [],
            "source": [
                "print(\"\\nPerforming cross-validation (FIXED - using Pipeline)...\")\n",
                "cv_scores_lr = cross_val_score(lr_pipeline, X_train, y_train, cv=5, n_jobs=-1)\n",
                "cv_scores_rf = cross_val_score(rf_pipeline, X_train, y_train, cv=5, n_jobs=-1)\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(10, 6))\n",
                "bp = ax.boxplot([cv_scores_lr, cv_scores_rf], \n",
                "                 labels=['Logistic Regression', 'Random Forest'],\n",
                "                 patch_artist=True, widths=0.5)\n",
                "\n",
                "colors = ['#3498db', '#2ecc71']\n",
                "for patch, color in zip(bp['boxes'], colors):\n",
                "    patch.set_facecolor(color)\n",
                "    patch.set_alpha(0.7)\n",
                "\n",
                "ax.set_ylabel('Accuracy Score', fontsize=12, weight='bold')\n",
                "ax.set_title('5-Fold Cross-Validation Results (FIXED Pipeline)', fontsize=14, weight='bold', pad=20)\n",
                "ax.grid(alpha=0.3, axis='y')\n",
                "\n",
                "means = [cv_scores_lr.mean(), cv_scores_rf.mean()]\n",
                "for i, mean in enumerate(means, 1):\n",
                "    ax.text(i, mean, f'Œº = {mean:.4f}', ha='center', va='bottom', \n",
                "            fontsize=10, weight='bold', bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('09_cross_validation.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "print(\"   ‚úÖ Saved: 09_cross_validation.png\")\n",
                "\n",
                "print(f\"\\n  CV Scores (Logistic Regression): {cv_scores_lr.mean():.4f} (+/- {cv_scores_lr.std():.4f})\")\n",
                "print(f\"  CV Scores (Random Forest):       {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std():.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "report_header"
            },
            "source": [
                "## Cell 17: Save Report & Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "save_report"
            },
            "outputs": [],
            "source": [
                "print(\"\\nSaving summary report...\")\n",
                "report = {\n",
                "    'dataset': {\n",
                "        'total_samples': len(df),\n",
                "        'features_used': 'URL-only (22 features)',\n",
                "        'feature_count': len(existing_url_features),\n",
                "        'phishing_samples': int(class_counts[0]),\n",
                "        'legitimate_samples': int(class_counts[1])\n",
                "    },\n",
                "    'pipeline': {\n",
                "        'fixed': True,\n",
                "        'uses_sklearn_pipeline': True,\n",
                "        'no_data_leakage': True\n",
                "    },\n",
                "    'models': {},\n",
                "    'cross_validation': {\n",
                "        'Logistic Regression': {'mean': float(cv_scores_lr.mean()), 'std': float(cv_scores_lr.std())},\n",
                "        'Random Forest': {'mean': float(cv_scores_rf.mean()), 'std': float(cv_scores_rf.std())}\n",
                "    }\n",
                "}\n",
                "\n",
                "for model_name, metrics in metrics_results.items():\n",
                "    report['models'][model_name] = {k: float(v) for k, v in metrics.items()}\n",
                "\n",
                "with open('training_report_fixed.json', 'w') as f:\n",
                "    json.dump(report, f, indent=2)\n",
                "print(\"   ‚úÖ Saved: training_report_fixed.json\")\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"‚úÖ TRAINING COMPLETE (FIXED PIPELINE - NO DATA LEAKAGE)\")\n",
                "print(\"=\"*80)\n",
                "print(\"\\nüîß FIXES APPLIED:\")\n",
                "print(\"  ‚úÖ StandardScaler wrapped inside sklearn Pipeline\")\n",
                "print(\"  ‚úÖ Cross-validation uses Pipeline (no leakage)\")\n",
                "print(\"  ‚úÖ Learning curve uses Pipeline (no leakage)\")\n",
                "print(\"  ‚úÖ URL-only features (22 features) for main evaluation\")\n",
                "print(\"\\nüìù NOTE: Metrics may be lower than before - this is CORRECT.\")\n",
                "print(\"   Previous high scores were due to data leakage.\")\n",
                "print(\"   These scores represent TRUE generalization performance.\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "download_header"
            },
            "source": [
                "## Cell 18: Download All Files"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "download_files"
            },
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "import glob\n",
                "\n",
                "print(\"üì• Downloading all generated files...\\n\")\n",
                "\n",
                "for png_file in glob.glob('*.png'):\n",
                "    print(f\"  Downloading: {png_file}\")\n",
                "    files.download(png_file)\n",
                "\n",
                "for pkl_file in ['lr_pipeline.pkl', 'rf_pipeline.pkl']:\n",
                "    print(f\"  Downloading: {pkl_file}\")\n",
                "    files.download(pkl_file)\n",
                "\n",
                "print(f\"  Downloading: training_report_fixed.json\")\n",
                "files.download('training_report_fixed.json')\n",
                "\n",
                "print(\"\\n‚úÖ All files downloaded!\")"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}