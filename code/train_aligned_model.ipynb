{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# GuardNet - Retrain Model (TF.js Compatible)\n",
                "\n",
                "Notebook ini akan:\n",
                "1. Membaca dataset PhiUSIIL\n",
                "2. Menghitung 50 fitur **PERSIS** seperti sandbox.js\n",
                "3. Train Logistic Regression\n",
                "4. Export ke TensorFlow.js (format kompatibel)\n",
                "\n",
                "**PENTING**: Menggunakan Keras 2.x untuk kompatibilitas dengan TensorFlow.js!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 1: FORCE KERAS 2.x (WAJIB DIJALANKAN PERTAMA!) ===\n",
                "# Ini HARUS dijalankan SEBELUM import tensorflow!\n",
                "import os\n",
                "os.environ['TF_USE_LEGACY_KERAS'] = '1'\n",
                "\n",
                "print('‚úÖ Keras 2.x legacy mode enabled')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 2: Install Dependencies ===\n",
                "!pip install tensorflowjs pandas scikit-learn tensorflow numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 3: Import Libraries ===\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import json\n",
                "import re\n",
                "import math\n",
                "from urllib.parse import urlparse\n",
                "import tensorflow as tf\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, classification_report\n",
                "\n",
                "print(f\"TensorFlow version: {tf.__version__}\")\n",
                "\n",
                "# Verifikasi menggunakan Keras 2.x\n",
                "if hasattr(tf.keras, 'layers') and hasattr(tf.keras.layers, 'Dense'):\n",
                "    print('‚úÖ Keras 2.x confirmed')\n",
                "else:\n",
                "    print('‚ö†Ô∏è Mungkin masih menggunakan Keras 3.x, restart runtime!')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 4: Load Dataset ===\n",
                "# OPSI 1: Upload langsung ke Colab (RECOMMENDED)\n",
                "from google.colab import files\n",
                "print('Upload file PhiUSIIL_Phishing_URL_Dataset.csv:')\n",
                "uploaded = files.upload()\n",
                "filename = list(uploaded.keys())[0]\n",
                "df = pd.read_csv(filename)\n",
                "\n",
                "# OPSI 2: Google Drive (uncomment jika pakai Drive)\n",
                "# from google.colab import drive\n",
                "# drive.mount('/content/drive')\n",
                "# df = pd.read_csv('/content/drive/MyDrive/PhiUSIIL_Phishing_URL_Dataset.csv')\n",
                "\n",
                "print(f\"\\nDataset shape: {df.shape}\")\n",
                "print(f\"Label distribution:\")\n",
                "print(df['label'].value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 5: Feature Extraction Functions (EXACT COPY dari sandbox.js) ===\n",
                "\n",
                "def count_char(s, char):\n",
                "    return s.count(char)\n",
                "\n",
                "def calculate_entropy(s):\n",
                "    if not s:\n",
                "        return 0\n",
                "    length = len(s)\n",
                "    frequencies = {}\n",
                "    for char in s:\n",
                "        frequencies[char] = frequencies.get(char, 0) + 1\n",
                "    entropy = 0\n",
                "    for char, freq in frequencies.items():\n",
                "        p = freq / length\n",
                "        entropy -= p * math.log2(p)\n",
                "    return entropy\n",
                "\n",
                "COMMON_TLDS = ['com', 'org', 'net', 'edu', 'gov', 'io', 'co', 'id']\n",
                "\n",
                "def extract_features_aligned(url_str):\n",
                "    try:\n",
                "        parsed = urlparse(url_str)\n",
                "        hostname = parsed.hostname or ''\n",
                "    except:\n",
                "        return [0] * 22\n",
                "    \n",
                "    features = []\n",
                "    \n",
                "    # 1. URLLength\n",
                "    url_length = len(url_str)\n",
                "    features.append(url_length)\n",
                "    \n",
                "    # 2. DomainLength\n",
                "    domain_length = len(hostname)\n",
                "    features.append(domain_length)\n",
                "    \n",
                "    # 3. IsDomainIP\n",
                "    is_ip = 1 if re.match(r'^(?:\\d{1,3}\\.){3}\\d{1,3}$', hostname) else 0\n",
                "    features.append(is_ip)\n",
                "    \n",
                "    # 4. URLSimilarityIndex (sandbox.js logic)\n",
                "    url_similarity = 80 if (url_length < 50 and domain_length < 20) else 50\n",
                "    features.append(url_similarity)\n",
                "    \n",
                "    # 5. CharContinuationRate\n",
                "    max_seq = 0\n",
                "    curr_seq = 1\n",
                "    for i in range(1, len(url_str)):\n",
                "        if url_str[i] == url_str[i-1]:\n",
                "            curr_seq += 1\n",
                "        else:\n",
                "            max_seq = max(max_seq, curr_seq)\n",
                "            curr_seq = 1\n",
                "    max_seq = max(max_seq, curr_seq)\n",
                "    char_continuation_rate = max_seq / url_length if url_length > 0 else 0\n",
                "    features.append(char_continuation_rate)\n",
                "    \n",
                "    # 6. TLDLegitimateProb\n",
                "    tld = hostname.split('.')[-1] if hostname else ''\n",
                "    tld_prob = 0.9 if tld in COMMON_TLDS else 0.3\n",
                "    features.append(tld_prob)\n",
                "    \n",
                "    # 7. URLCharProb\n",
                "    url_entropy = calculate_entropy(url_str)\n",
                "    url_char_prob = 1.0 / (url_entropy + 1)\n",
                "    features.append(url_char_prob)\n",
                "    \n",
                "    # 8. TLDLength\n",
                "    features.append(len(tld))\n",
                "    \n",
                "    # 9. NoOfSubDomain\n",
                "    parts = hostname.split('.') if hostname else []\n",
                "    num_subdomains = max(0, len(parts) - 2)\n",
                "    features.append(num_subdomains)\n",
                "    \n",
                "    # 10. HasObfuscation\n",
                "    has_obfuscation = 1 if re.search(r'%[0-9A-Fa-f]{2}', url_str) else 0\n",
                "    features.append(has_obfuscation)\n",
                "    \n",
                "    # 11. NoOfObfuscatedChar\n",
                "    num_obfuscated = len(re.findall(r'%[0-9A-Fa-f]{2}', url_str))\n",
                "    features.append(num_obfuscated)\n",
                "    \n",
                "    # 12. ObfuscationRatio\n",
                "    obfuscation_ratio = num_obfuscated / url_length if url_length > 0 else 0\n",
                "    features.append(obfuscation_ratio)\n",
                "    \n",
                "    # 13. NoOfLettersInURL\n",
                "    num_letters = len(re.findall(r'[a-zA-Z]', url_str))\n",
                "    features.append(num_letters)\n",
                "    \n",
                "    # 14. LetterRatioInURL\n",
                "    letter_ratio = num_letters / url_length if url_length > 0 else 0\n",
                "    features.append(letter_ratio)\n",
                "    \n",
                "    # 15. NoOfDigitsInURL\n",
                "    num_digits = len(re.findall(r'\\d', url_str))\n",
                "    features.append(num_digits)\n",
                "    \n",
                "    # 16. DigitRatioInURL\n",
                "    digit_ratio = num_digits / url_length if url_length > 0 else 0\n",
                "    features.append(digit_ratio)\n",
                "    \n",
                "    # 17. NoOfEqualsInURL\n",
                "    features.append(count_char(url_str, '='))\n",
                "    \n",
                "    # 18. NoOfQMarkInURL\n",
                "    features.append(count_char(url_str, '?'))\n",
                "    \n",
                "    # 19. NoOfAmpersandInURL\n",
                "    features.append(count_char(url_str, '&'))\n",
                "    \n",
                "    # 20. NoOfOtherSpecialCharsInURL\n",
                "    num_special = len(re.findall(r'[^a-zA-Z0-9\\s]', url_str))\n",
                "    features.append(num_special)\n",
                "    \n",
                "    # 21. SpecialCharRatioInURL\n",
                "    special_ratio = num_special / url_length if url_length > 0 else 0\n",
                "    features.append(special_ratio)\n",
                "    \n",
                "    # 22. IsHTTPS\n",
                "    is_https = 1 if url_str.startswith('https://') else 0\n",
                "    features.append(is_https)\n",
                "    \n",
                "    return features\n",
                "\n",
                "print(\"‚úÖ Feature extraction functions defined.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 6: Define Feature Names ===\n",
                "URL_FEATURES = [\n",
                "    'URLLength', 'DomainLength', 'IsDomainIP', 'URLSimilarityIndex',\n",
                "    'CharContinuationRate', 'TLDLegitimateProb', 'URLCharProb', 'TLDLength',\n",
                "    'NoOfSubDomain', 'HasObfuscation', 'NoOfObfuscatedChar', 'ObfuscationRatio',\n",
                "    'NoOfLettersInURL', 'LetterRatioInURL', 'NoOfDegitsInURL', 'DegitRatioInURL',\n",
                "    'NoOfEqualsInURL', 'NoOfQMarkInURL', 'NoOfAmpersandInURL',\n",
                "    'NoOfOtherSpecialCharsInURL', 'SpacialCharRatioInURL', 'IsHTTPS'\n",
                "]\n",
                "\n",
                "CONTENT_FEATURES = [\n",
                "    'LineOfCode', 'LargestLineLength', 'HasTitle', 'DomainTitleMatchScore',\n",
                "    'URLTitleMatchScore', 'HasFavicon', 'Robots', 'IsResponsive',\n",
                "    'NoOfURLRedirect', 'NoOfSelfRedirect', 'HasDescription', 'NoOfPopup',\n",
                "    'NoOfiFrame', 'HasExternalFormSubmit', 'HasSocialNet', 'HasSubmitButton',\n",
                "    'HasHiddenFields', 'HasPasswordField', 'Bank', 'Pay', 'Crypto',\n",
                "    'HasCopyrightInfo', 'NoOfImage', 'NoOfCSS', 'NoOfJS', 'NoOfSelfRef',\n",
                "    'NoOfEmptyRef', 'NoOfExternalRef'\n",
                "]\n",
                "\n",
                "ALL_FEATURES = URL_FEATURES + CONTENT_FEATURES\n",
                "print(f\"Total features: {len(ALL_FEATURES)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 7: Extract URL Features ===\n",
                "print(\"Computing URL features with aligned extraction...\")\n",
                "\n",
                "url_features_list = []\n",
                "for idx, row in df.iterrows():\n",
                "    url = row['URL']\n",
                "    features = extract_features_aligned(url)\n",
                "    url_features_list.append(features)\n",
                "    if idx % 10000 == 0:\n",
                "        print(f\"Processed {idx}/{len(df)} rows...\")\n",
                "\n",
                "url_features_df = pd.DataFrame(url_features_list, columns=URL_FEATURES)\n",
                "print(f\"\\n‚úÖ URL features computed: {url_features_df.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 8: Combine Features ===\n",
                "content_df = df[CONTENT_FEATURES].copy()\n",
                "X_combined = pd.concat([url_features_df.reset_index(drop=True), \n",
                "                        content_df.reset_index(drop=True)], axis=1)\n",
                "X_combined = X_combined.fillna(0)\n",
                "\n",
                "X = X_combined.values\n",
                "y = df['label'].values\n",
                "\n",
                "print(f\"X shape: {X.shape}\")\n",
                "print(f\"y shape: {y.shape}\")\n",
                "print(f\"Class distribution: 0={sum(y==0)}, 1={sum(y==1)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 9: Train-Test Split ===\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "print(f\"Training set: {X_train.shape}\")\n",
                "print(f\"Test set: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 10: StandardScaler ===\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "# Save scaler params\n",
                "scaler_params = {\n",
                "    'mean': scaler.mean_.tolist(),\n",
                "    'std': scaler.scale_.tolist(),\n",
                "    'feature_names': ALL_FEATURES\n",
                "}\n",
                "\n",
                "with open('scaler_params.json', 'w') as f:\n",
                "    json.dump(scaler_params, f, indent=2)\n",
                "\n",
                "print(\"‚úÖ Scaler parameters saved\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 11: Train Logistic Regression ===\n",
                "sklearn_lr = LogisticRegression(\n",
                "    max_iter=2000, \n",
                "    class_weight='balanced', \n",
                "    solver='lbfgs',\n",
                "    random_state=42\n",
                ")\n",
                "\n",
                "sklearn_lr.fit(X_train_scaled, y_train)\n",
                "\n",
                "y_pred = sklearn_lr.predict(X_test_scaled)\n",
                "y_proba = sklearn_lr.predict_proba(X_test_scaled)[:, 1]\n",
                "\n",
                "print(f\"\\n=== Model Evaluation ===\")\n",
                "print(f\"Accuracy  : {accuracy_score(y_test, y_pred):.5f}\")\n",
                "print(f\"Precision : {precision_score(y_test, y_pred):.5f}\")\n",
                "print(f\"Recall    : {recall_score(y_test, y_pred):.5f}\")\n",
                "print(f\"F1-Score  : {f1_score(y_test, y_pred):.5f}\")\n",
                "print(f\"ROC-AUC   : {roc_auc_score(y_test, y_proba):.5f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 12: Build Keras 2.x Model ===\n",
                "# PENTING: Ini menggunakan Keras 2.x API\n",
                "keras_model = tf.keras.Sequential([\n",
                "    tf.keras.layers.InputLayer(input_shape=(50,)),\n",
                "    tf.keras.layers.Dense(1, activation='sigmoid', name='dense')\n",
                "])\n",
                "\n",
                "# Transfer weights dari sklearn\n",
                "sklearn_weights = sklearn_lr.coef_.T\n",
                "sklearn_bias = sklearn_lr.intercept_\n",
                "keras_model.layers[0].set_weights([sklearn_weights, sklearn_bias])\n",
                "\n",
                "keras_model.compile(\n",
                "    optimizer='adam',\n",
                "    loss='binary_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "keras_model.summary()\n",
                "print(\"\\n‚úÖ Keras 2.x model created\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 13: Verify Keras Model ===\n",
                "y_pred_keras = (keras_model.predict(X_test_scaled) > 0.5).astype(int).flatten()\n",
                "match_rate = (y_pred_keras == y_pred).mean()\n",
                "print(f\"Prediction match rate with sklearn: {match_rate*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 14: Export to TensorFlow.js ===\n",
                "import tensorflowjs as tfjs\n",
                "\n",
                "os.makedirs('tfjs_model', exist_ok=True)\n",
                "tfjs.converters.save_keras_model(keras_model, 'tfjs_model')\n",
                "\n",
                "print(\"\\n‚úÖ TensorFlow.js model saved\")\n",
                "print(\"\\nFiles created:\")\n",
                "for f in os.listdir('tfjs_model'):\n",
                "    size = os.path.getsize(f'tfjs_model/{f}')\n",
                "    print(f\"  - {f} ({size} bytes)\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 15: Verify model.json format ===\n",
                "with open('tfjs_model/model.json', 'r') as f:\n",
                "    model_config = json.load(f)\n",
                "\n",
                "# Check for Keras 2.x format\n",
                "topology = model_config.get('modelTopology', {})\n",
                "keras_version = topology.get('keras_version', 'unknown')\n",
                "print(f\"Keras version in export: {keras_version}\")\n",
                "\n",
                "# Check InputLayer config\n",
                "layers = topology.get('model_config', {}).get('config', {}).get('layers', [])\n",
                "if layers:\n",
                "    first_layer = layers[0]\n",
                "    config = first_layer.get('config', {})\n",
                "    if 'batch_input_shape' in config:\n",
                "        print(\"‚úÖ Format compatible: batch_input_shape found\")\n",
                "    elif 'batch_shape' in config:\n",
                "        print(\"‚ö†Ô∏è Keras 3.x format detected: batch_shape - RESTART RUNTIME!\")\n",
                "    else:\n",
                "        print(f\"Layer config: {config}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 16: Copy scaler and create zip ===\n",
                "import shutil\n",
                "shutil.copy('scaler_params.json', 'tfjs_model/scaler_params.json')\n",
                "\n",
                "!cd tfjs_model && zip -r ../guardnet_model_v2.zip .\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"‚úÖ SEMUA FILE SIAP!\")\n",
                "print(\"=\"*50)\n",
                "for f in os.listdir('tfjs_model'):\n",
                "    print(f\"  üìÑ {f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === Cell 17: Download ===\n",
                "from google.colab import files\n",
                "files.download('guardnet_model_v2.zip')\n",
                "\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"LANGKAH SELANJUTNYA:\")\n",
                "print(\"=\"*60)\n",
                "print(\"1. Extract guardnet_model_v2.zip\")\n",
                "print(\"2. Copy SEMUA file ke folder GuardNet/models/:\")\n",
                "print(\"   - model.json\")\n",
                "print(\"   - group1-shard1of1.bin\")\n",
                "print(\"   - scaler_params.json\")\n",
                "print(\"3. Reload extension di chrome://extensions\")\n",
                "print(\"4. Test!\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}